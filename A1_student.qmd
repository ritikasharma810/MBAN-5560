---
title: "Assignment 1 - Basics of Hyperparameter Tuning: Finding the Optimal Span & Degree for LOESS"
subtitle: "MBAN 5560 - Due January 31, 2026 (Saturday) 11:59pm"
author: "Dr. Aydede"
date: today
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    theme: cosmo
execute:
  echo: true
  warning: false
  message: false
---

  
In this assignment, you will apply the self-learning methods (cross-validation and bootstrapping) covered in class (January 27) to tune the `span` and `degree` hyperparameters for LOESS regression. Your goal is to find the optimal span that minimizes prediction error and to quantify the uncertainty in your results.

**Key Learning Objectives:**

1. Implement grid search for hyperparameter tuning
2. Compare simple train-test split vs k-fold CV vs bootstrap CV
3. Report prediction error with uncertainty (mean and SD)
4. Benchmark tuned LOESS against simpler models

**Important Notes:**

- You can team up with **two classmates** for this assignment (maximum 3 students per team). Submit one assignment per team.
- Use R and Quarto for your analysis. Submit the rendered HTML file along with the QMD source file.
- Make sure your code runs without errors and produces the expected outputs.
- Provide interpretations and explanations for your results, not just code outputs.
- Using LLM assistance is allowed, but you must disclose which tool you used and how it helped.

---

# Setup

Load the required libraries:

```{r setup}
# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
```

---

# Part A: Data Setup and Exploration (15 points)

## A.1 Simulated Data

We will use simulated data where the true relationship is known but complex:

$$f(x) = 50 + 15x - 0.3x^2 + 30\sin(x/3) + 10\cos(x)$$

This combines a quadratic trend with multiple periodic components, making it challenging to predict.

```{r simulate-data}
# Generate simulated data
set.seed(5560)
n <- 500
x <- sort(runif(n, min = 0, max = 30))
f_true <- 50 + 15*x - 0.3*x^2 + 30*sin(x/3) + 10*cos(x)
y <- f_true + rnorm(n, mean = 0, sd = 15)
data <- data.frame(y = y, x = x, f_true = f_true)
```

## Task A.1: Visualize the data

Create a scatter plot of the data with the true function overlaid as a red dashed line.

```{r plot-data}
# YOUR CODE HERE: Create scatter plot with true function
ggplot(data, aes(x = x, y = y)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_line(aes(y = f_true), color = "red", linetype = "dashed", linewidth = 1) +
  labs(title = "Simulated Data with True Function",
       subtitle = "Red dashed line shows the true underlying relationship",
       x = "X", y = "Y") +
  theme_minimal()


```

#### **Question 1 (3 points):** Describe the pattern you see in the data. What makes this relationship challenging for a simple linear model?

**Your Answer:**
The data shows a complex non-linear pattern that combines a quadratic trend (upward curve) with periodic oscillations (waves). The relationship is not a straight line - it curves upward and also oscillates up and down in a wave-like pattern. A simple linear model would fail because it can only fit a straight line, which cannot capture either the curvature or the periodic waves. The model would have large prediction errors across the entire range of X values.

---

## Task A.2: Explore different span values

Fit LOESS models with three different span values (0.2, 0.5, 0.8) using `degree = 1`. Plot all three fits on the same graph along with the true function.

```{r explore-spans}
# YOUR CODE HERE: Fit LOESS with span = 0.2, 0.5, 0.8 and plot
# Fit LOESS with different spans
fit_02 <- loess(y ~ x, data = data, span = 0.2, degree = 1)
fit_05 <- loess(y ~ x, data = data, span = 0.5, degree = 1)
fit_08 <- loess(y ~ x, data = data, span = 0.8, degree = 1)

# Add predictions to data
data_plot <- data %>%
  mutate(pred_02 = predict(fit_02),
         pred_05 = predict(fit_05),
         pred_08 = predict(fit_08))

# Plot all fits
ggplot(data_plot, aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3, color = "gray") +
  geom_line(aes(y = f_true, color = "True Function"), linetype = "dashed", linewidth = 1.2) +
  geom_line(aes(y = pred_02, color = "Span 0.2"), linewidth = 1) +
  geom_line(aes(y = pred_05, color = "Span 0.5"), linewidth = 1) +
  geom_line(aes(y = pred_08, color = "Span 0.8"), linewidth = 1) +
  scale_color_manual(values = c("True Function" = "red", 
                                 "Span 0.2" = "blue", 
                                 "Span 0.5" = "green", 
                                 "Span 0.8" = "purple")) +
  labs(title = "LOESS Fits with Different Span Values",
       subtitle = "Comparing flexibility vs smoothness",
       x = "X", y = "Y", color = "Model") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

#### **Question 2 (4 points):** How does the span parameter affect the fitted curve? Which span appears to best capture the true relationship?

**Your Answer:**
The span parameter controls how much data is used for local fitting. Span 0.2 (blue line) uses only 20% of nearby points, creating a very wiggly, flexible curve that follows every small fluctuation - it appears to be overfitting to noise. Span 0.8 (purple line) uses 80% of points, creating a very smooth curve that misses the periodic waves - it is underfitting. Span 0.5 (green line) appears to best capture the true relationship (red dashed line) by balancing flexibility and smoothness, following the general pattern without fitting the noise.

#### **Question 3 (4 points):** Explain the bias-variance tradeoff in the context of the span parameter. Which span has high bias? Which has high variance?

**Your Answer:**
Span 0.2 has HIGH VARIANCE - it's too flexible and changes dramatically with small changes in the training data. It fits the random noise, leading to different predictions on new data (poor generalization). Span 0.8 has HIGH BIAS - it's too rigid and systematically misses the true pattern, underfitting regardless of which data sample we use. The bias-variance tradeoff means we need a middle ground: span 0.5 balances these two error sources, achieving lower total prediction error by being flexible enough to capture the pattern but not so flexible that it fits noise.

#### **Question 4 (4 points):** Can you determine the optimal span just by looking at these plots? Why or why not?

**Your Answer:**
No, we cannot reliably determine the optimal span just by visual inspection. We are looking at how well the models fit the TRAINING data, but what matters is prediction on NEW, unseen data. A model might look perfect on training data but perform poorly on test data (overfitting). We need a systematic validation approach like cross-validation that evaluates prediction error on held-out data. Only by testing on data the model hasn't seen can we measure true predictive performance and find the optimal span.
---

# Part B: Simple Train-Test Split (20 points)

## Task B.1: Single train-test split

Implement a grid search to find the optimal span using a single 80/20 train-test split.

**Requirements:**
- Use `degree = 1` (fixed)
- Search span values from 0.1 to 0.9 by 0.05
- Calculate RMSPE on the test set for each span value
- Report the optimal span and its RMSPE

```{r single-split}
# Create hyperparameter grid
grid_span <- seq(from = 0.1, to = 0.9, by = 0.05)

# YOUR CODE HERE: Implement single train-test split grid search
# 1. Set seed to 100
# 2. Split data 80/20
# 3. For each span, fit LOESS and calculate RMSPE
# 4. Find optimal span

# Set seed for reproducibility
set.seed(100)

# Split data 80/20
train_idx <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

# Create results storage
results <- data.frame(span = grid_span, rmspe = NA)

# Grid search - try each span value
for(i in 1:length(grid_span)) {
  # Fit model on training data
  fit <- loess(y ~ x, data = train_data, span = grid_span[i], degree = 1)
  
  # Predict on test data
  pred <- predict(fit, newdata = test_data)
  
  # Calculate RMSPE (Root Mean Squared Prediction Error)
  results$rmspe[i] <- sqrt(mean((test_data$y - pred)^2))
}

# Find optimal span
optimal_span <- results$span[which.min(results$rmspe)]
optimal_rmspe <- min(results$rmspe)

# Display results
cat("Optimal span:", optimal_span, "\n")
cat("Test RMSPE:", round(optimal_rmspe, 2), "\n")

# Plot RMSPE vs span
ggplot(results, aes(x = span, y = rmspe)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  geom_vline(xintercept = optimal_span, linetype = "dashed", color = "red") +
  labs(title = "RMSPE vs Span (Single Train-Test Split)",
       subtitle = paste("Optimal span =", optimal_span),
       x = "Span", y = "RMSPE") +
  theme_minimal()
```

#### **Question 5 (5 points):** What is the optimal span found with seed = 100? What is the corresponding RMSPE?

**Your Answer:**
With seed = 100, the optimal span is 0.15 and the corresponding test RMSPE is 14.01. This span minimizes prediction error on the held-out test set for this particular random split. The plot shows that very small spans (0.1-0.2) perform best, while larger spans (above 0.5) have much higher prediction errors.
---

## Task B.2: Demonstrate instability

Repeat the grid search with three different seeds (200, 300, 400) to show how the optimal span varies.

```{r instability-demo}
# YOUR CODE HERE: Run grid search with seeds 200, 300, 400
# Store the optimal span and RMSPE for each
# Test with different seeds

# Test with different seeds
seeds <- c(200, 300, 400)
seed_results <- data.frame(seed = seeds, optimal_span = NA, rmspe = NA)

for(s in 1:length(seeds)) {
  set.seed(seeds[s])
  
  # Split data
  train_idx <- sample(1:nrow(data), size = 0.8 * nrow(data))
  train_data <- data[train_idx, ]
  test_data <- data[-train_idx, ]
  
  # Grid search
  results_temp <- data.frame(span = grid_span, rmspe = NA)
  
  for(i in 1:length(grid_span)) {
    # Fit model
    fit <- loess(y ~ x, data = train_data, span = grid_span[i], degree = 1)
    
    # Predict (with error handling)
    pred <- predict(fit, newdata = test_data)
    
    # Only calculate RMSPE if no NA values
    if(!any(is.na(pred))) {
      results_temp$rmspe[i] <- sqrt(mean((test_data$y - pred)^2))
    }
  }
  
  # Find best span (excluding NAs)
  valid_results <- results_temp[!is.na(results_temp$rmspe), ]
  
  if(nrow(valid_results) > 0) {
    best_idx <- which.min(valid_results$rmspe)
    seed_results$optimal_span[s] <- valid_results$span[best_idx]
    seed_results$rmspe[s] <- valid_results$rmspe[best_idx]
  }
}

# Display results
print(seed_results)
cat("\nSpan range:", min(seed_results$optimal_span, na.rm=TRUE), "to", 
    max(seed_results$optimal_span, na.rm=TRUE), "\n")
cat("RMSPE range:", round(min(seed_results$rmspe, na.rm=TRUE), 2), "to", 
    round(max(seed_results$rmspe, na.rm=TRUE), 2), "\n")


```

#### **Question 6 (5 points):** Report the optimal span found with each seed. How much does it vary?

**Your Answer:**
The optimal spans found are:
- Seed 200: span = 0.15, RMSPE = 17.93
- Seed 300: Failed (no valid predictions)
- Seed 400: span = 0.10, RMSPE = 14.31

The span varies between 0.10 and 0.15 for the successful splits. Even though the range seems small, this represents a 50% difference in the proportion of data used for local fitting (10% vs 15%), which can significantly affect the model's flexibility.

#### **Question 7 (5 points):** Why does the optimal span change with different random splits? What does this tell us about using a single train-test split for hyperparameter tuning?

**Your Answer:**
The optimal span changes because each random split creates a different training and test set. Different splits may have different patterns or noise in the test set, leading to different "optimal" spans. One split even failed completely (seed 300), showing extreme instability. This tells us that a single train-test split is unreliable for hyperparameter tuning - the results are highly dependent on luck (which random split you get). We cannot trust that the optimal span from one split will generalize well.

#### **Question 8 (5 points):** Based on this exercise, would you trust the optimal span from a single split? What approach would be more reliable?

**Your Answer:**
No, I would not trust the optimal span from a single split. We saw that different splits gave different optimal spans (0.10 vs 0.15), different RMSPEs (14.31 vs 17.93), and one split even failed completely. A more reliable approach is cross-validation, where we use multiple train-test splits and average the results. This reduces the impact of random variation and gives us a more stable estimate of which hyperparameter performs best on average across different data splits.

---

# Part C: 10-Fold Cross-Validation (25 points)

## Task C.1: Implement nested CV

Implement proper nested cross-validation with the **correct loop structure**:

- **Outer loop:** 10 iterations with random 80/20 splits creating `modata` (model data) and `test` set
- **Inner structure:** For EACH hyperparameter, calculate mean RMSPE across k folds, then select
- Use `degree = 1`
- Report the selected span and test RMSPE for each outer iteration

**IMPORTANT: Loop Order Matters!**

The hyperparameter loop must be **OUTSIDE** and the CV folds loop must be **INSIDE**:

```
for each hyperparameter setting:     # OUTER - loop over grid
    for each fold i in 1:k:          # INNER - loop over CV folds
        train on k-1 folds, validate on fold i
    average RMSPE across all k folds for THIS hyperparameter
select hyperparameter with lowest average RMSPE
```

This ensures we average errors FIRST, then select - NOT select per-fold then average selections!

```{r nested-cv}
# YOUR CODE HERE: Implement nested 10-fold CV

```

#### **Question 9 (5 points):** Create a table showing the selected span and test RMSPE for each of the 10 outer iterations.

**Your Answer:**

#### **Question 10 (5 points):** What is the mean and standard deviation of the test RMSPE across the 10 outer iterations?

**Your Answer:**

#### **Question 11 (5 points):** Is the selected span consistent across iterations? What span would you recommend?

**Your Answer:**

---

## Task C.2: Extend to tuning both span AND degree

Now tune both `span` and `degree` (1 or 2) using the same nested CV structure.

```{r nested-cv-both}
# Create grid with both span and degree
grid_both <- expand.grid(
  span = seq(from = 0.1, to = 0.9, by = 0.1),
  degree = c(1, 2)
)

# YOUR CODE HERE: Implement nested CV tuning both span and degree


```

#### **Question 12 (5 points):** What combination of span and degree gives the best results? Does tuning degree in addition to span improve prediction?

**Your Answer:**

#### **Question 13 (5 points):** Compare the mean RMSPE from tuning only span (degree=1) vs tuning both. Is the improvement meaningful?

**Your Answer:**

---

# Part D: Bootstrap Cross-Validation (25 points)

## Task D.1: Implement Bootstrap CV

Implement bootstrap cross-validation with OOB validation using the **correct loop structure**:

- **Outer loop:** 8 iterations with random 80/20 splits creating `modata` (model data) and `test` set
- **Inner structure:** For EACH hyperparameter, run B bootstrap iterations and average RMSPE, then select
- Use the same grid as Part C (both span and degree)
- Report results for each outer iteration

**IMPORTANT: Loop Order Matters!**

Same principle as k-fold CV - hyperparameter loop **OUTSIDE**, bootstrap loop **INSIDE**:

```
for each hyperparameter setting:     # OUTER - loop over grid
    for j in 1:B:                    # INNER - loop over bootstrap iterations
        sample with replacement → train set
        OOB observations → validation set
        calculate RMSPE
    average RMSPE across all B iterations for THIS hyperparameter
select hyperparameter with lowest average RMSPE
```

```{r bootstrap-cv}
# YOUR CODE HERE: Implement bootstrap CV

```

#### **Question 14 (5 points):** Report the mean and SD of test RMSPE from bootstrap CV.

**Your Answer:**

#### **Question 15 (5 points):** Compare the bootstrap CV results to the k-fold CV results from Part C. Which method gave more stable results (lower SD)?

**Your Answer:**

#### **Question 16 (5 points):** The OOB validation set is approximately what percentage of the data? How does this compare to 10-fold CV where each validation fold is 10% of the data?

**Your Answer:**

#### **Question 17 (5 points):** Based on your results, which method would you recommend for hyperparameter tuning: k-fold CV or bootstrap CV? Justify your choice.

**Your Answer:**

#### **Question 18 (5 points):** Did bootstrap CV select different hyperparameters than k-fold CV? Explain any differences.

**Your Answer:**

---

# Part E: Benchmark Comparison (15 points)

## Task E.1: Fit benchmark models

Using the same nested CV structure (10 outer iterations), evaluate these benchmark models:
1. **Linear regression** (no tuning needed)
2. **Polynomial regression (degree 4)** (no tuning needed)
3. **LOESS with default span (0.75) and degree 1** (no tuning needed)

Compare with your **tuned LOESS** results from Part C.

```{r benchmarks}
# YOUR CODE HERE: Evaluate benchmark models using same 10 outer iterations
# For each model, calculate mean and SD of test RMSPE


```

#### **Question 19 (5 points):** Create a summary table comparing all methods with columns: Model, Mean RMSPE, SD RMSPE

**Your Answer:**

#### **Question 20 (5 points):** How much did hyperparameter tuning improve RMSPE compared to using the default span (0.75)?

**Your Answer:**

#### **Question 21 (5 points):** Based on your benchmark comparison, which model would you recommend for this data? Consider both accuracy (mean RMSPE) and reliability (SD). Is the improvement from tuning worth the computational cost?

**Your Answer:**

---

# Submission Checklist

- [ ] All code chunks completed and running without errors
- [ ] All 21 questions answered with complete explanations
- [ ] Summary tables created for each part
- [ ] Clear interpretations connecting results to self-learning concepts
- [ ] Team members listed in author field
- [ ] LLM usage disclosed (if applicable)
- [ ] Document renders to HTML successfully

---

**Good luck with your analysis!**
