---
title: "Assignment 1 - Basics of Hyperparameter Tuning: Finding the Optimal Span & Degree for LOESS"
subtitle: "MBAN 5560 - Due January 31, 2026 (Saturday) 11:59pm"
author: "Dr. Aydede"
date: today
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    theme: cosmo
execute:
  echo: true
  warning: false
  message: false
---

  
In this assignment, you will apply the self-learning methods (cross-validation and bootstrapping) covered in class (January 27) to tune the `span` and `degree` hyperparameters for LOESS regression. Your goal is to find the optimal span that minimizes prediction error and to quantify the uncertainty in your results.

**Key Learning Objectives:**

1. Implement grid search for hyperparameter tuning
2. Compare simple train-test split vs k-fold CV vs bootstrap CV
3. Report prediction error with uncertainty (mean and SD)
4. Benchmark tuned LOESS against simpler models

**Important Notes:**

- You can team up with **two classmates** for this assignment (maximum 3 students per team). Submit one assignment per team.
- Use R and Quarto for your analysis. Submit the rendered HTML file along with the QMD source file.
- Make sure your code runs without errors and produces the expected outputs.
- Provide interpretations and explanations for your results, not just code outputs.
- Using LLM assistance is allowed, but you must disclose which tool you used and how it helped.

---

# Setup

Load the required libraries:

```{r setup}
# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
```

---

# Part A: Data Setup and Exploration (15 points)

## A.1 Simulated Data

We will use simulated data where the true relationship is known but complex:

$$f(x) = 50 + 15x - 0.3x^2 + 30\sin(x/3) + 10\cos(x)$$

This combines a quadratic trend with multiple periodic components, making it challenging to predict.

```{r simulate-data}
# Generate simulated data
set.seed(5560)
n <- 500
x <- sort(runif(n, min = 0, max = 30))
f_true <- 50 + 15*x - 0.3*x^2 + 30*sin(x/3) + 10*cos(x)
y <- f_true + rnorm(n, mean = 0, sd = 15)
data <- data.frame(y = y, x = x, f_true = f_true)
```

## Task A.1: Visualize the data

Create a scatter plot of the data with the true function overlaid as a red dashed line.

```{r plot-data}
# YOUR CODE HERE: Create scatter plot with true function
ggplot(data, aes(x = x, y = y)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_line(aes(y = f_true), color = "red", linetype = "dashed", linewidth = 1) +
  labs(title = "Simulated Data with True Function",
       subtitle = "Red dashed line shows the true underlying relationship",
       x = "X", y = "Y") +
  theme_minimal()


```

#### **Question 1 (3 points):** Describe the pattern you see in the data. What makes this relationship challenging for a simple linear model?

**Your Answer:**
The data shows a complex non-linear pattern that combines a quadratic trend (upward curve) with periodic oscillations (waves). The relationship is not a straight line - it curves upward and also oscillates up and down in a wave-like pattern. A simple linear model would fail because it can only fit a straight line, which cannot capture either the curvature or the periodic waves. The model would have large prediction errors across the entire range of X values.

---

## Task A.2: Explore different span values

Fit LOESS models with three different span values (0.2, 0.5, 0.8) using `degree = 1`. Plot all three fits on the same graph along with the true function.

```{r explore-spans}
# YOUR CODE HERE: Fit LOESS with span = 0.2, 0.5, 0.8 and plot
# Fit LOESS with different spans
fit_02 <- loess(y ~ x, data = data, span = 0.2, degree = 1)
fit_05 <- loess(y ~ x, data = data, span = 0.5, degree = 1)
fit_08 <- loess(y ~ x, data = data, span = 0.8, degree = 1)

# Add predictions to data
data_plot <- data %>%
  mutate(pred_02 = predict(fit_02),
         pred_05 = predict(fit_05),
         pred_08 = predict(fit_08))

# Plot all fits
ggplot(data_plot, aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3, color = "gray") +
  geom_line(aes(y = f_true, color = "True Function"), linetype = "dashed", linewidth = 1.2) +
  geom_line(aes(y = pred_02, color = "Span 0.2"), linewidth = 1) +
  geom_line(aes(y = pred_05, color = "Span 0.5"), linewidth = 1) +
  geom_line(aes(y = pred_08, color = "Span 0.8"), linewidth = 1) +
  scale_color_manual(values = c("True Function" = "red", 
                                 "Span 0.2" = "blue", 
                                 "Span 0.5" = "green", 
                                 "Span 0.8" = "purple")) +
  labs(title = "LOESS Fits with Different Span Values",
       subtitle = "Comparing flexibility vs smoothness",
       x = "X", y = "Y", color = "Model") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

#### **Question 2 (4 points):** How does the span parameter affect the fitted curve? Which span appears to best capture the true relationship?

**Your Answer:**
The span parameter controls how much data is used for local fitting. Span 0.2 (blue line) uses only 20% of nearby points, creating a very wiggly, flexible curve that follows every small fluctuation - it appears to be overfitting to noise. Span 0.8 (purple line) uses 80% of points, creating a very smooth curve that misses the periodic waves - it is underfitting. Span 0.5 (green line) appears to best capture the true relationship (red dashed line) by balancing flexibility and smoothness, following the general pattern without fitting the noise.

#### **Question 3 (4 points):** Explain the bias-variance tradeoff in the context of the span parameter. Which span has high bias? Which has high variance?

**Your Answer:**
Span 0.2 has HIGH VARIANCE - it's too flexible and changes dramatically with small changes in the training data. It fits the random noise, leading to different predictions on new data (poor generalization). Span 0.8 has HIGH BIAS - it's too rigid and systematically misses the true pattern, underfitting regardless of which data sample we use. The bias-variance tradeoff means we need a middle ground: span 0.5 balances these two error sources, achieving lower total prediction error by being flexible enough to capture the pattern but not so flexible that it fits noise.

#### **Question 4 (4 points):** Can you determine the optimal span just by looking at these plots? Why or why not?

**Your Answer:**
No, we cannot reliably determine the optimal span just by visual inspection. We are looking at how well the models fit the TRAINING data, but what matters is prediction on NEW, unseen data. A model might look perfect on training data but perform poorly on test data (overfitting). We need a systematic validation approach like cross-validation that evaluates prediction error on held-out data. Only by testing on data the model hasn't seen can we measure true predictive performance and find the optimal span.
---

# Part B: Simple Train-Test Split (20 points)

## Task B.1: Single train-test split

Implement a grid search to find the optimal span using a single 80/20 train-test split.

**Requirements:**
- Use `degree = 1` (fixed)
- Search span values from 0.1 to 0.9 by 0.05
- Calculate RMSPE on the test set for each span value
- Report the optimal span and its RMSPE

```{r single-split}
# Create hyperparameter grid
grid_span <- seq(from = 0.1, to = 0.9, by = 0.05)

# YOUR CODE HERE: Implement single train-test split grid search
# 1. Set seed to 100
# 2. Split data 80/20
# 3. For each span, fit LOESS and calculate RMSPE
# 4. Find optimal span

# Set seed for reproducibility
set.seed(100)

# Split data 80/20
train_idx <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

# Create results storage
results <- data.frame(span = grid_span, rmspe = NA)

# Grid search - try each span value
for(i in 1:length(grid_span)) {
  # Fit model on training data
  fit <- loess(y ~ x, data = train_data, span = grid_span[i], degree = 1)
  
  # Predict on test data
  pred <- predict(fit, newdata = test_data)
  
  # Calculate RMSPE (Root Mean Squared Prediction Error)
  results$rmspe[i] <- sqrt(mean((test_data$y - pred)^2))
}

# Find optimal span
optimal_span <- results$span[which.min(results$rmspe)]
optimal_rmspe <- min(results$rmspe)

# Display results
cat("Optimal span:", optimal_span, "\n")
cat("Test RMSPE:", round(optimal_rmspe, 2), "\n")

# Plot RMSPE vs span
ggplot(results, aes(x = span, y = rmspe)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  geom_vline(xintercept = optimal_span, linetype = "dashed", color = "red") +
  labs(title = "RMSPE vs Span (Single Train-Test Split)",
       subtitle = paste("Optimal span =", optimal_span),
       x = "Span", y = "RMSPE") +
  theme_minimal()
```

#### **Question 5 (5 points):** What is the optimal span found with seed = 100? What is the corresponding RMSPE?

**Your Answer:**
With seed = 100, the optimal span is 0.15 and the corresponding test RMSPE is 14.01. This span minimizes prediction error on the held-out test set for this particular random split. The plot shows that very small spans (0.1-0.2) perform best, while larger spans (above 0.5) have much higher prediction errors.
---

## Task B.2: Demonstrate instability

Repeat the grid search with three different seeds (200, 300, 400) to show how the optimal span varies.

```{r instability-demo}
# YOUR CODE HERE: Run grid search with seeds 200, 300, 400
# Store the optimal span and RMSPE for each
# Test with different seeds

# Test with different seeds
seeds <- c(200, 300, 400)
seed_results <- data.frame(seed = seeds, optimal_span = NA, rmspe = NA)

for(s in 1:length(seeds)) {
  set.seed(seeds[s])
  
  # Split data
  train_idx <- sample(1:nrow(data), size = 0.8 * nrow(data))
  train_data <- data[train_idx, ]
  test_data <- data[-train_idx, ]
  
  # Grid search
  results_temp <- data.frame(span = grid_span, rmspe = NA)
  
  for(i in 1:length(grid_span)) {
    # Fit model
    fit <- loess(y ~ x, data = train_data, span = grid_span[i], degree = 1)
    
    # Predict (with error handling)
    pred <- predict(fit, newdata = test_data)
    
    # Only calculate RMSPE if no NA values
    if(!any(is.na(pred))) {
      results_temp$rmspe[i] <- sqrt(mean((test_data$y - pred)^2))
    }
  }
  
  # Find best span (excluding NAs)
  valid_results <- results_temp[!is.na(results_temp$rmspe), ]
  
  if(nrow(valid_results) > 0) {
    best_idx <- which.min(valid_results$rmspe)
    seed_results$optimal_span[s] <- valid_results$span[best_idx]
    seed_results$rmspe[s] <- valid_results$rmspe[best_idx]
  }
}

# Display results
print(seed_results)
cat("\nSpan range:", min(seed_results$optimal_span, na.rm=TRUE), "to", 
    max(seed_results$optimal_span, na.rm=TRUE), "\n")
cat("RMSPE range:", round(min(seed_results$rmspe, na.rm=TRUE), 2), "to", 
    round(max(seed_results$rmspe, na.rm=TRUE), 2), "\n")


```

#### **Question 6 (5 points):** Report the optimal span found with each seed. How much does it vary?

**Your Answer:**
The optimal spans found are:
- Seed 200: span = 0.15, RMSPE = 17.93
- Seed 300: Failed (no valid predictions)
- Seed 400: span = 0.10, RMSPE = 14.31

The span varies between 0.10 and 0.15 for the successful splits. Even though the range seems small, this represents a 50% difference in the proportion of data used for local fitting (10% vs 15%), which can significantly affect the model's flexibility.

#### **Question 7 (5 points):** Why does the optimal span change with different random splits? What does this tell us about using a single train-test split for hyperparameter tuning?

**Your Answer:**
The optimal span changes because each random split creates a different training and test set. Different splits may have different patterns or noise in the test set, leading to different "optimal" spans. One split even failed completely (seed 300), showing extreme instability. This tells us that a single train-test split is unreliable for hyperparameter tuning - the results are highly dependent on luck (which random split you get). We cannot trust that the optimal span from one split will generalize well.

#### **Question 8 (5 points):** Based on this exercise, would you trust the optimal span from a single split? What approach would be more reliable?

**Your Answer:**
No, I would not trust the optimal span from a single split. We saw that different splits gave different optimal spans (0.10 vs 0.15), different RMSPEs (14.31 vs 17.93), and one split even failed completely. A more reliable approach is cross-validation, where we use multiple train-test splits and average the results. This reduces the impact of random variation and gives us a more stable estimate of which hyperparameter performs best on average across different data splits.

---

# Part C: 10-Fold Cross-Validation (25 points)

## Task C.1: Implement nested CV

Implement proper nested cross-validation with the **correct loop structure**:

- **Outer loop:** 10 iterations with random 80/20 splits creating `modata` (model data) and `test` set
- **Inner structure:** For EACH hyperparameter, calculate mean RMSPE across k folds, then select
- Use `degree = 1`
- Report the selected span and test RMSPE for each outer iteration

**IMPORTANT: Loop Order Matters!**

The hyperparameter loop must be **OUTSIDE** and the CV folds loop must be **INSIDE**:

```
for each hyperparameter setting:     # OUTER - loop over grid
    for each fold i in 1:k:          # INNER - loop over CV folds
        train on k-1 folds, validate on fold i
    average RMSPE across all k folds for THIS hyperparameter
select hyperparameter with lowest average RMSPE
```

This ensures we average errors FIRST, then select - NOT select per-fold then average selections!

```{r nested-cv}
# YOUR CODE HERE: Implement nested 10-fold CV
# Nested Cross-Validation
set.seed(5560)
n_outer <- 10
k_folds <- 10

# Storage for results
outer_results <- data.frame(
  iteration = 1:n_outer, 
  selected_span = NA, 
  test_rmspe = NA
)

# Outer loop: 10 iterations with different train/test splits
for(outer_iter in 1:n_outer) {
  
  # Create outer split (80/20)
  test_idx <- sample(1:nrow(data), size = 0.2 * nrow(data))
  modata <- data[-test_idx, ]  # Model data (80%)
  test_data <- data[test_idx, ]  # Test data (20%)
  
  # For each span, calculate average RMSPE across k folds
  grid_rmspe <- rep(NA, length(grid_span))
  
  # OUTER LOOP: Try each span value
  for(g in 1:length(grid_span)) {
    
    # INNER LOOP: K-fold CV for this span
    fold_rmspe <- rep(NA, k_folds)
    fold_idx <- sample(rep(1:k_folds, length.out = nrow(modata)))
    
    for(fold in 1:k_folds) {
      # Split modata into train and validation
      val_data <- modata[fold_idx == fold, ]
      train_data <- modata[fold_idx != fold, ]
      
      # Fit model
      fit <- loess(y ~ x, data = train_data, span = grid_span[g], degree = 1)
      
      # Predict on validation fold
      pred <- predict(fit, newdata = val_data)
      
      # Calculate RMSPE for this fold
      if(!any(is.na(pred))) {
        fold_rmspe[fold] <- sqrt(mean((val_data$y - pred)^2))
      }
    }
    
    # Average RMSPE across all folds for THIS span
    grid_rmspe[g] <- mean(fold_rmspe, na.rm = TRUE)
  }
  
  # Select span with lowest average RMSPE
  best_span <- grid_span[which.min(grid_rmspe)]
  
  # Refit on all modata and test on held-out test set
  final_fit <- loess(y ~ x, data = modata, span = best_span, degree = 1)
  final_pred <- predict(final_fit, newdata = test_data)
  
  # Store results
  outer_results$selected_span[outer_iter] <- best_span
  if(!any(is.na(final_pred))) {
    outer_results$test_rmspe[outer_iter] <- sqrt(mean((test_data$y - final_pred)^2))
  }
  
  cat("Iteration", outer_iter, "complete - Selected span:", best_span, "\n")
}

# Summary statistics
cat("\n=== RESULTS ===\n")
print(outer_results)
cat("\nMean Test RMSPE:", round(mean(outer_results$test_rmspe, na.rm = TRUE), 2), "\n")
cat("SD Test RMSPE:", round(sd(outer_results$test_rmspe, na.rm = TRUE), 2), "\n")
cat("Most common span:", names(sort(table(outer_results$selected_span), decreasing = TRUE))[1], "\n")

```

#### **Question 9 (5 points):** Create a table showing the selected span and test RMSPE for each of the 10 outer iterations.

**Your Answer:**
Here is the table of results from 10-fold nested cross-validation:

| Iteration | Selected Span | Test RMSPE |
|-----------|---------------|------------|
| 1         | 0.25          | 15.40      |
| 2         | 0.15          | 14.54      |
| 3         | 0.10          | NA         |
| 4         | 0.15          | NA         |
| 5         | 0.15          | 16.00      |
| 6         | 0.15          | 17.44      |
| 7         | 0.10          | NA         |
| 8         | 0.10          | NA         |
| 9         | 0.15          | 16.23      |
| 10        | 0.10          | 17.18      |

Note: Some iterations have NA for test RMSPE due to prediction failures on the test set.

#### **Question 10 (5 points):** What is the mean and standard deviation of the test RMSPE across the 10 outer iterations?

**Your Answer:**
Mean Test RMSPE: 16.13
SD Test RMSPE: 1.09

This shows that on average, our tuned LOESS model has a prediction error of about 16.13 units, with relatively low variability (SD = 1.09) across different test sets.

#### **Question 11 (5 points):** Is the selected span consistent across iterations? What span would you recommend?

**Your Answer:**
The selected span is fairly consistent. Out of 10 iterations:
- Span 0.15 was selected 5 times (most common)
- Span 0.10 was selected 4 times
- Span 0.25 was selected 1 time

I would recommend span = 0.15 because it was selected most frequently and represents a good balance between the very flexible 0.10 and the smoother 0.25. The consistency across iterations (most values between 0.10-0.15) gives us much more confidence than the unstable results from single train-test splits in Part B.

---

## Task C.2: Extend to tuning both span AND degree

Now tune both `span` and `degree` (1 or 2) using the same nested CV structure.

```{r nested-cv-both}
# Create grid with both span and degree
grid_both <- expand.grid(
  span = seq(from = 0.1, to = 0.9, by = 0.1),
  degree = c(1, 2)
)

# YOUR CODE HERE: Implement nested CV tuning both span and degree
# Nested CV tuning both span and degree
set.seed(5560)

outer_results_both <- data.frame(
  iteration = 1:n_outer,
  selected_span = NA,
  selected_degree = NA,
  test_rmspe = NA
)

for(outer_iter in 1:n_outer) {
  
  # Outer split
  test_idx <- sample(1:nrow(data), size = 0.2 * nrow(data))
  modata <- data[-test_idx, ]
  test_data <- data[test_idx, ]
  
  # For each hyperparameter combo, calculate average RMSPE
  grid_rmspe <- rep(NA, nrow(grid_both))
  
  for(g in 1:nrow(grid_both)) {
    
    # K-fold CV for this combo
    fold_rmspe <- rep(NA, k_folds)
    fold_idx <- sample(rep(1:k_folds, length.out = nrow(modata)))
    
    for(fold in 1:k_folds) {
      val_data <- modata[fold_idx == fold, ]
      train_data <- modata[fold_idx != fold, ]
      
      fit <- loess(y ~ x, data = train_data, 
                   span = grid_both$span[g], 
                   degree = grid_both$degree[g])
      pred <- predict(fit, newdata = val_data)
      
      if(!any(is.na(pred))) {
        fold_rmspe[fold] <- sqrt(mean((val_data$y - pred)^2))
      }
    }
    
    grid_rmspe[g] <- mean(fold_rmspe, na.rm = TRUE)
  }
  
  # Select best combo
  best_idx <- which.min(grid_rmspe)
  best_span <- grid_both$span[best_idx]
  best_degree <- grid_both$degree[best_idx]
  
  # Test on held-out set
  final_fit <- loess(y ~ x, data = modata, span = best_span, degree = best_degree)
  final_pred <- predict(final_fit, newdata = test_data)
  
  outer_results_both$selected_span[outer_iter] <- best_span
  outer_results_both$selected_degree[outer_iter] <- best_degree
  if(!any(is.na(final_pred))) {
    outer_results_both$test_rmspe[outer_iter] <- sqrt(mean((test_data$y - final_pred)^2))
  }
  
  cat("Iteration", outer_iter, "- Span:", best_span, "Degree:", best_degree, "\n")
}

cat("\n=== RESULTS (SPAN + DEGREE) ===\n")
print(outer_results_both)
cat("\nMean Test RMSPE:", round(mean(outer_results_both$test_rmspe, na.rm = TRUE), 2), "\n")
cat("SD Test RMSPE:", round(sd(outer_results_both$test_rmspe, na.rm = TRUE), 2), "\n")

```

#### **Question 12 (5 points):** What combination of span and degree gives the best results? Does tuning degree in addition to span improve prediction?

**Your Answer:**
The most common combination is span = 0.2 with degree = 2 (selected 5 times out of 10). Span = 0.2 with degree = 1 was also selected 3 times. Yes, tuning degree in addition to span does improve prediction - the mean test RMSPE improved from 16.13 (degree=1 only) to 14.53 (tuning both), a reduction of about 1.6 units in prediction error. The quadratic local fits (degree=2) appear to better capture the complex patterns in the data.
#### **Question 13 (5 points):** Compare the mean RMSPE from tuning only span (degree=1) vs tuning both. Is the improvement meaningful?

**Your Answer:**
Comparison:
- Tuning only span (degree=1): Mean RMSPE = 16.13, SD = 1.09
- Tuning both span and degree: Mean RMSPE = 14.53, SD = 0.66

The improvement is meaningful: prediction error decreased by 1.6 units (about 10% improvement). Additionally, the standard deviation also decreased from 1.09 to 0.66, indicating MORE STABLE predictions across different test sets. This suggests that allowing degree=2 helps the model better capture the true non-linear pattern, leading to both better and more consistent predictions.
---

# Part D: Bootstrap Cross-Validation (25 points)

## Task D.1: Implement Bootstrap CV

Implement bootstrap cross-validation with OOB validation using the **correct loop structure**:

- **Outer loop:** 8 iterations with random 80/20 splits creating `modata` (model data) and `test` set
- **Inner structure:** For EACH hyperparameter, run B bootstrap iterations and average RMSPE, then select
- Use the same grid as Part C (both span and degree)
- Report results for each outer iteration

**IMPORTANT: Loop Order Matters!**

Same principle as k-fold CV - hyperparameter loop **OUTSIDE**, bootstrap loop **INSIDE**:

```
for each hyperparameter setting:     # OUTER - loop over grid
    for j in 1:B:                    # INNER - loop over bootstrap iterations
        sample with replacement → train set
        OOB observations → validation set
        calculate RMSPE
    average RMSPE across all B iterations for THIS hyperparameter
select hyperparameter with lowest average RMSPE
```

```{r bootstrap-cv}
# YOUR CODE HERE: Implement bootstrap CV
# Bootstrap Cross-Validation
set.seed(5560)
n_outer_boot <- 8  # 8 outer iterations
B <- 25  # 25 bootstrap iterations per hyperparameter

# Storage for results
outer_results_boot <- data.frame(
  iteration = 1:n_outer_boot,
  selected_span = NA,
  selected_degree = NA,
  test_rmspe = NA
)

# Outer loop: 8 iterations
for(outer_iter in 1:n_outer_boot) {
  
  # Outer split (80/20)
  test_idx <- sample(1:nrow(data), size = 0.2 * nrow(data))
  modata <- data[-test_idx, ]
  test_data <- data[test_idx, ]
  
  # For each hyperparameter combo, calculate average bootstrap RMSPE
  grid_rmspe_boot <- rep(NA, nrow(grid_both))
  
  # OUTER: Try each hyperparameter combination
  for(g in 1:nrow(grid_both)) {
    
    boot_rmspe <- rep(NA, B)
    
    # INNER: Bootstrap iterations for this hyperparameter
    for(b in 1:B) {
      
      # Bootstrap sample (sample WITH replacement)
      boot_idx <- sample(1:nrow(modata), size = nrow(modata), replace = TRUE)
      train_boot <- modata[boot_idx, ]
      
      # Out-of-bag (OOB) observations = validation set
      oob_idx <- setdiff(1:nrow(modata), unique(boot_idx))
      
      if(length(oob_idx) > 0) {
        val_boot <- modata[oob_idx, ]
        
        # Fit model on bootstrap sample
        fit_boot <- loess(y ~ x, data = train_boot,
                          span = grid_both$span[g],
                          degree = grid_both$degree[g])
        
        # Predict on OOB
        pred_boot <- predict(fit_boot, newdata = val_boot)
        
        # Calculate RMSPE
        if(!any(is.na(pred_boot))) {
          boot_rmspe[b] <- sqrt(mean((val_boot$y - pred_boot)^2))
        }
      }
    }
    
    # Average RMSPE across all B bootstrap iterations
    grid_rmspe_boot[g] <- mean(boot_rmspe, na.rm = TRUE)
  }
  
  # Select best hyperparameter combination
  best_idx <- which.min(grid_rmspe_boot)
  best_span <- grid_both$span[best_idx]
  best_degree <- grid_both$degree[best_idx]
  
  # Test on held-out test set
  final_fit <- loess(y ~ x, data = modata, span = best_span, degree = best_degree)
  final_pred <- predict(final_fit, newdata = test_data)
  
  # Store results
  outer_results_boot$selected_span[outer_iter] <- best_span
  outer_results_boot$selected_degree[outer_iter] <- best_degree
  if(!any(is.na(final_pred))) {
    outer_results_boot$test_rmspe[outer_iter] <- sqrt(mean((test_data$y - final_pred)^2))
  }
  
  cat("Iteration", outer_iter, "- Span:", best_span, "Degree:", best_degree, "\n")
}

cat("\n=== BOOTSTRAP CV RESULTS ===\n")
print(outer_results_boot)
cat("\nMean Test RMSPE:", round(mean(outer_results_boot$test_rmspe, na.rm = TRUE), 2), "\n")
cat("SD Test RMSPE:", round(sd(outer_results_boot$test_rmspe, na.rm = TRUE), 2), "\n")

```

#### **Question 14 (5 points):** Report the mean and SD of test RMSPE from bootstrap CV.

**Your Answer:**
Mean Test RMSPE: 14.99
SD Test RMSPE: 0.08

The bootstrap CV gives very similar mean error to k-fold CV (14.99 vs 14.53) but with much lower standard deviation (0.08 vs 0.66), indicating extremely stable results.

#### **Question 15 (5 points):** Compare the bootstrap CV results to the k-fold CV results from Part C. Which method gave more stable results (lower SD)?

**Your Answer:**
Bootstrap CV gave MORE STABLE results:
- K-fold CV: Mean = 14.53, SD = 0.66
- Bootstrap CV: Mean = 14.99, SD = 0.08

Bootstrap CV has dramatically lower standard deviation (0.08 vs 0.66), meaning the test error is much more consistent across different iterations. The mean errors are similar (both around 14-15), but bootstrap provides far less variability in the results.

#### **Question 16 (5 points):** The OOB validation set is approximately what percentage of the data? How does this compare to 10-fold CV where each validation fold is 10% of the data?

**Your Answer:**
In bootstrap sampling, approximately 36.8% of observations are Out-of-Bag (OOB) and not selected in the bootstrap sample. This is because when sampling with replacement, the probability of NOT being selected is (1-1/n)^n ≈ 1/e ≈ 0.368 as n gets large.

In 10-fold CV, each validation fold is exactly 10% of the data. So bootstrap uses a LARGER validation set (37% vs 10%), which may contribute to more stable error estimates since we're validating on more data each time.
#### **Question 17 (5 points):** Based on your results, which method would you recommend for hyperparameter tuning: k-fold CV or bootstrap CV? Justify your choice.

**Your Answer:**
I would recommend BOOTSTRAP CV for the following reasons:

1. **Much more stable:** SD of 0.08 vs 0.66 means we can trust the results more
2. **Larger validation sets:** Using ~37% OOB data vs 10% per fold gives more reliable error estimates
3. **Consistent selections:** Bootstrap consistently chose span=0.2, degree=2 (7 out of 8 times)

However, k-fold CV is computationally simpler and still reliable. For critical applications where stability matters most, bootstrap is better. For quick experimentation, k-fold CV is sufficient.

#### **Question 18 (5 points):** Did bootstrap CV select different hyperparameters than k-fold CV? Explain any differences.

**Your Answer:**
Bootstrap CV and k-fold CV selected very similar hyperparameters:
- K-fold CV: Most common was span=0.2, degree=2
- Bootstrap CV: span=0.2, degree=2 selected 7 out of 8 times

The methods agree on the optimal hyperparameters! Both consistently prefer span=0.2 with degree=2, giving us high confidence that this is truly the best combination. The slight difference is that bootstrap was MORE CONSISTENT in its selection, showing less variation across iterations.
---

# Part E: Benchmark Comparison (15 points)

## Task E.1: Fit benchmark models

Using the same nested CV structure (10 outer iterations), evaluate these benchmark models:
1. **Linear regression** (no tuning needed)
2. **Polynomial regression (degree 4)** (no tuning needed)
3. **LOESS with default span (0.75) and degree 1** (no tuning needed)

Compare with your **tuned LOESS** results from Part C.

```{r benchmarks}
# YOUR CODE HERE: Evaluate benchmark models using same 10 outer iterations
# For each model, calculate mean and SD of test RMSPE


```

#### **Question 19 (5 points):** Create a summary table comparing all methods with columns: Model, Mean RMSPE, SD RMSPE

**Your Answer:**

#### **Question 20 (5 points):** How much did hyperparameter tuning improve RMSPE compared to using the default span (0.75)?

**Your Answer:**

#### **Question 21 (5 points):** Based on your benchmark comparison, which model would you recommend for this data? Consider both accuracy (mean RMSPE) and reliability (SD). Is the improvement from tuning worth the computational cost?

**Your Answer:**

---

# Submission Checklist

- [ ] All code chunks completed and running without errors
- [ ] All 21 questions answered with complete explanations
- [ ] Summary tables created for each part
- [ ] Clear interpretations connecting results to self-learning concepts
- [ ] Team members listed in author field
- [ ] LLM usage disclosed (if applicable)
- [ ] Document renders to HTML successfully

---

**Good luck with your analysis!**
